{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e6042d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Généralités sur la résolution de systèmes linéaires\n",
    "\n",
    "Il est fondamental de noter que résoudre (on dit aussi, à tort,\n",
    "inverser) un système n'implique pas de calculer la matrice inverse.\n",
    "Calculer une inverse revient à résoudre $n$ systèmes (un par vecteur de\n",
    "base) et est donc une opération très coûteuse et souvent inutile.\n",
    "\n",
    "Les résultats donnés par la suite dans ${\\mathbb{C}}$ restent\n",
    "valables dans ${\\mathbb{R}}$ (toutes les matrices impliquées\n",
    "sont alors réelles).\n",
    "\n",
    "On veut résoudre\n",
    "${\\mathbf{A}}{\\mathbf{x}}={\\mathbf{b}}$.\n",
    "On introduit des bases des noyaux à droite et à gauche\n",
    "${\\mathbf{A}}{\\mathbf{N}}_d=0$ et\n",
    "${\\mathbf{A}}^H{\\mathbf{N}}_g=0$.\n",
    "\n",
    "On rappelle l'alternative de Fredholm :\n",
    "\n",
    "-   ${\\mathbf{b}}\\in{\\operatorname{Im}({\\mathbf{A}})}\\ \\Leftrightarrow\\ {\\mathbf{N}}_g^H {\\mathbf{b}} = 0$\n",
    "    alors le problème a une solution définie à un élément de\n",
    "    ${\\operatorname{Ker}_d({\\mathbf{A}})}$ près\n",
    "    (que l'on peut noter\n",
    "    ${\\mathbf{N}}_d {\\boldsymbol{\\alpha}}$ avec\n",
    "    ${\\boldsymbol{\\alpha}}$ quelconque). Si\n",
    "    ${\\operatorname{Ker}_d({\\mathbf{A}})}\\neq\\{0\\}$,\n",
    "    on parle de problème sous-déterminé.\n",
    "\n",
    "-   ${\\mathbf{b}}\\notin{\\operatorname{Im}({\\mathbf{A}})}\\ \\Leftrightarrow\\ {\\mathbf{N}}_g^H {\\mathbf{b}} \\neq 0$\n",
    "    alors le problème n'a pas de solution, on parle de problème\n",
    "    surdéterminé.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b637165",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Remarquons qu'il arrive que le second membre ne soit pas entièrement\n",
    "connu et que cette partie inconnue serve à rendre le problème bien posé\n",
    ": on résout en fait\n",
    "${\\mathbf{A}}{\\mathbf{x}}={\\mathbf{b}}+{\\mathbf{y}}$\n",
    "où ${\\mathbf{y}}$ est inconnu et doit satisfaire\n",
    "${\\mathbf{N}}_g^H ({\\mathbf{b}} +{\\mathbf{y}})=0$\n",
    "; la détermination de ${\\mathbf{y}}$ est un problème\n",
    "sous-déterminé dont on parlera plus tard.\n",
    "\n",
    "Dans le cas des systèmes rectangulaires, on sait qu'il y a\n",
    "nécessairement des noyaux à gauche ou à droite et que soit il n'y a pas\n",
    "de solution (en général) soit la solution n'est pas unique.\n",
    "\n",
    "On verra que la résolution de systèmes rectangulaires implique (au moins\n",
    "formellement) la résolution de systèmes carrés\n",
    "${\\mathbf{A}}^H {\\mathbf{C}} {\\mathbf{A}}$,\n",
    "où ${\\mathbf{C}}$ est une matrice symétrique définie\n",
    "positive, on a le résultat fondamental suivant :\n",
    "\n",
    "\n",
    "**Proposition 4.1**. *Pour toute matrice ${\\mathbf{A}}$,\n",
    "${\\mathbf{C}}$ étant une matrice symétrique (hermitienne)\n",
    "définie positive, on a\n",
    "${\\operatorname{Ker}({\\mathbf{A^H C A}})}={\\operatorname{Ker}({\\mathbf{A}})}$\n",
    "et\n",
    "${\\operatorname{Im}({\\mathbf{A^H C A}})}={\\operatorname{Im}({\\mathbf{A^H}})}$.*\n",
    "\n",
    "\n",
    "*Proof.* On a bien sûr\n",
    "${\\operatorname{Ker}({\\mathbf{A}})}\\subset {\\operatorname{Ker}({\\mathbf{A^H C A}})}$\n",
    "; si on prend un vecteur ${\\mathbf{x}}$ de\n",
    "${\\operatorname{Ker}({\\mathbf{A^H C A}})}$ alors\n",
    "$0={\\mathbf{x}}^H{\\mathbf{A}}^H {\\mathbf{C}}{\\mathbf{A}}{\\mathbf{x}}=\\|{\\mathbf{A}}{\\mathbf{x}}\\|_{{\\mathbf{C}}}\\Rightarrow {\\mathbf{x}}\\in{\\operatorname{Ker}({\\mathbf{A}})}$.\n",
    "\n",
    "On a bien sûr\n",
    "${\\operatorname{Im}({\\mathbf{A^H C A}})} \\subset {\\operatorname{Im}({\\mathbf{A^H}})}$\n",
    "; si\n",
    "${\\mathbf{y}}\\in{\\operatorname{Im}({\\mathbf{A^H}})}$\n",
    "alors\n",
    "$\\exists {\\mathbf{x}}/ {\\mathbf{A}}^H {\\mathbf{x}} = {\\mathbf{y}}$.\n",
    "D'après la supplémentarité image/noyau, il existe des uniques\n",
    "$({\\mathbf{x}}_1,{\\mathbf{x}}_2)$ tels que\n",
    "${\\mathbf{x}}={\\mathbf{x}}_1+{\\mathbf{x}}_2$\n",
    "avec\n",
    "${\\mathbf{x}}_1\\in{\\operatorname{Im}({\\mathbf{CA}})},\\  {\\mathbf{x}}_1={\\mathbf{CA}} {\\mathbf{z}}$\n",
    "et\n",
    "${\\mathbf{x}}_2\\in{\\operatorname{Ker}({\\mathbf{A^HC}})}$,\n",
    "on a donc\n",
    "${\\mathbf{y}}={\\mathbf{A}}^H{\\mathbf{C}}{\\mathbf{A}} {\\mathbf{z}}$\n",
    "et l'inclusion est réciproque. ◻\n",
    "\n",
    "**Définition 4.1**. On dira d'une matrice qu'elle est de rang plein si\n",
    "son rang est égal à la plus petite de ses dimensions. Dans ce cas un des\n",
    "noyaux (à gauche ou à droite suivant la forme de la matrice) est réduit\n",
    "à $0$. En anglais suivant la forme de la matrice, on parlera de\n",
    "*full-column-ranked matrix* ou *full-row-ranked matrix*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c356e4ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Problème surdéterminé\n",
    "\n",
    "Si\n",
    "${\\mathbf{b}}\\notin{\\operatorname{Im}({\\mathbf{A}})}$,\n",
    "ce qui implique\n",
    "${\\operatorname{Ker}_g({\\mathbf{A}})}\\neq \\{0\\}$\n",
    "et ce qui est typiquement le cas si la matrice est rectangulaire\n",
    "$m\\times n$ avec plus de lignes ($m$) que de colonnes ($n<m$, pas assez\n",
    "d'inconnues, trop d'équations), le problème peut être compris dans un\n",
    "sens étendu, et on cherche en fait à minimiser un résidu :\n",
    "${\\mathbf{x}}=\\arg\\min_{{\\mathbf{y}}}\\| {\\mathbf{b}} - {\\mathbf{A}}{\\mathbf{y}} \\|$.\n",
    "\n",
    "Le résultat dépend du choix de la norme.\n",
    "$${\\operatorname{argmin}}\\left\\| \\begin{pmatrix}\n",
    "1 \\\\ 1 \\\\ 1 \n",
    "\\end{pmatrix}\n",
    "x - \\begin{pmatrix}\n",
    "b_1 \\\\b_2 \\\\ b_3\n",
    "\\end{pmatrix}\n",
    "\\right\\|_p \n",
    "\\qquad \\text{avec }b_1\\geqslant b_2\\geqslant b_3\\qquad \\left\\{\\begin{array}{ll}p=1&x=(b_1+b_3)/2 \\\\p=2&x=(b_1+b_2+b_3)/3\\\\p=\\infty&x=b_2\\\\\\end{array}\\right.$$\n",
    "\n",
    "Resoudre le problème de minimisation proposé pour les 3 normes en utilisant la fonction optimize de scipy. On prendra $b_1=5, b_2=2, b_3=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1eda0cd1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assertions True\n",
      "\n",
      "Norm 1 of the function:  [2.] = b2 = [2]\n",
      "Norm 2 of the function:  [2.667] = b1+b2+b3/3 =  [2.667]\n",
      "Norm inf of the function:  [3.] = b1+b3/2 =  [3.]\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import numpy.linalg as la\n",
    "\n",
    "# Generate the arrays\n",
    "A = np.ones((3,1))\n",
    "b = np.array([[5],[2],[1]])\n",
    "\n",
    "# Confirmation with a random array\n",
    "# b = np.random.random((3,1))\n",
    "\n",
    "def red_norm(n):\n",
    "    '''Minimization of the function `fun` '''\n",
    "    \n",
    "    # Function using lambda\n",
    "    fun = lambda x: la.norm(A*x - b, n)\n",
    "    \n",
    "    # Minimization using scipy, without displaying convergence results\n",
    "    mins = sc.optimize.fmin(fun, 1, disp = 0)\n",
    "    return mins\n",
    "\n",
    "# Calculations of b\n",
    "\n",
    "b123 = (b[1]+b[2]+b[0])/3\n",
    "b13 = (b[2]+b[0])/2\n",
    "# Assertions\n",
    "assert(np.allclose(np.round(red_norm(1),3), np.round(b[1], 3),1e-6))\n",
    "assert(np.allclose(np.round(red_norm(2),3), np.round(b123, 3),1e-6))\n",
    "assert(np.allclose(np.round(red_norm(np.inf),3), np.round(b13, 3),1e-6))\n",
    "print('Assertions True\\n')\n",
    "print(\"Norm 1 of the function: \", np.round(red_norm(1),3), '= b2 =', np.round(b[1], 3))\n",
    "print(\"Norm 2 of the function: \", np.round(red_norm(2),3), '= b1+b2+b3/3 = ', np.round(b123,3))\n",
    "print(\"Norm inf of the function: \", np.round(red_norm(np.inf),3), '= b1+b3/2 = ', np.round(b13,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485ca709",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Les normes $1$ et $\\infty$ étant non différentiables elles sont plus\n",
    "difficiles à manipuler. On préfère donc en général la norme $2$ qui\n",
    "conduit aux problèmes de moindres carrés, ou une norme associée à une\n",
    "matrice ${\\mathbf{C}}$ symétrique (hermitienne) définie\n",
    "positive.\n",
    "\n",
    "La différentiation de l'expression à minimiser conduit à :\n",
    "$$\\begin{gathered}\n",
    "0 = d\\|{\\mathbf{b}}-{\\mathbf{A}}{\\mathbf{x}}\\|_C^2=d\\left(({\\mathbf{b}}-{\\mathbf{A}}{\\mathbf{x}})^T{\\mathbf{C}}({\\mathbf{b}}-{\\mathbf{A}}{\\mathbf{x}})\\right)\\\\= d\\left(({\\mathbf{b}}^T{\\mathbf{C}}{\\mathbf{b}}+{\\mathbf{x}}^T{\\mathbf{A}}^T{\\mathbf{C}}{\\mathbf{A}}{\\mathbf{x}}-2{\\mathbf{x}}^T{\\mathbf{A}}^T{\\mathbf{C}}{\\mathbf{b}})\\right)=2(d{\\mathbf{x}})^T\\left({\\mathbf{A}}^T{\\mathbf{C}}{\\mathbf{A}}{\\mathbf{x}}-{\\mathbf{A}}^T{\\mathbf{C}}{\\mathbf{b}}\\right)\n",
    "\\end{gathered}$$ Donc le problème de minimisation se ramène à résoudre\n",
    "${\\mathbf{A}}^T{\\mathbf{C}}{\\mathbf{A}}{\\mathbf{x}}={\\mathbf{A}}^T{\\mathbf{C}}{\\mathbf{b}}$\n",
    "qui est un problème carré bien posé dans le sens où le second membre est\n",
    "dans l'image de l'opérateur. Quand\n",
    "${\\mathbf{C}}={\\mathbf{I}}$ on parle d'équation\n",
    "normale. La matrice\n",
    "${\\mathbf{A}}^T{\\mathbf{C}}{\\mathbf{A}}$\n",
    "est symétrique positive, quand ${\\mathbf{A}}$ est de rang\n",
    "plein elle est définie et se prête bien à une factorisation de Choleski.\n",
    "Le problème d'une telle approche est le mauvais conditionnement puisque\n",
    "${\\kappa_{2}(\\mathbf{A^T A})}= {\\kappa_{2}(\\mathbf{A})}^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a220f2b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problème sous-déterminé\n",
    "\n",
    "Quitte à étudier l'équation normale d'un problème surdéterminé, on se\n",
    "place dans le cas où\n",
    "${\\mathbf{b}}\\in{\\operatorname{Im}({\\mathbf{A}})}$\n",
    "mais\n",
    "$\\dim({\\operatorname{Ker}_d({\\mathbf{A}})})>0$, le\n",
    "problème est dit sous-déterminé, il existe une infinité de solution.\n",
    "C'est typiquement le cas des matrices avec plus de colonne que de lignes\n",
    "(pas assez d'équations).\n",
    "\n",
    "Pour rendre le problème à solution unique, on peut chercher à minimiser\n",
    "la norme du vecteur solution :\n",
    "${\\mathbf{x}}=\\arg\\min_{{\\mathbf{A}}{\\mathbf{y}}={\\mathbf{b}}}\\|{\\mathbf{y}}\\|$.\n",
    "La minimisation se faisant sur le noyau à droite qui est un sous-espace\n",
    "vectoriel et donc un ensemble convexe, on sait qu'il existe un unique\n",
    "élément de norme minimale. En général on choisit une norme définie par\n",
    "une matrice ${\\mathbf{C}}$ symétrique (hermitienne) définie\n",
    "positive. On introduit un Lagrangien pour réaliser la minimisation sous\n",
    "contrainte:\n",
    "$$\\mathcal{L}({\\mathbf{y}},\\boldsymbol{\\mu}) = \\|{\\mathbf{y}}\\|_{{\\mathbf{C}}}^2 + \\left({\\mathbf{A}}{\\mathbf{y}}-{\\mathbf{b}}\\right)^T\\boldsymbol{\\mu}$$\n",
    "dont la stationnarité est réalisée\n",
    "$({\\mathbf{x}},{\\boldsymbol{\\lambda}})$ donnés par\n",
    ":\n",
    "$$\\left({\\mathbf{A}} {\\mathbf{C}}^{-1}{\\mathbf{A}}^T \\right) {\\boldsymbol{\\lambda}}= -{\\mathbf{b}} \\qquad \\text{et}\\qquad \n",
    "{\\mathbf{x}} = -{\\mathbf{C}}^{-1}{\\mathbf{A}}^T {\\boldsymbol{\\lambda}}$$\n",
    "D'après l'hypothèse de départ\n",
    "${\\mathbf{b}}\\in{\\operatorname{Im}({\\mathbf{A}})}$,\n",
    "le problème carré de matrice symétrique positive\n",
    "$\\left({\\mathbf{A}} {\\mathbf{C}}^{-1}{\\mathbf{A}}^T \\right)$\n",
    "est bien posé, le problème est défini si ${\\mathbf{A}}$ est\n",
    "de rang plein. Notons que si la matrice n'est pas de rang plein\n",
    "${\\boldsymbol{\\lambda}}$ n'est pas défini de manière unique\n",
    "mais ${\\mathbf{x}}$ l'est.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6c168e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Élimination par blocs - complément de Schur\n",
    "\n",
    "Soit le découpage par blocs suivant :\n",
    "$${\\mathbf{A}}=\\begin{pmatrix}{\\mathbf{A}}_{11}&{\\mathbf{A}}_{12}\\\\{\\mathbf{A}}_{21}&{\\mathbf{A}}_{22}\\end{pmatrix}$$\n",
    "où ${\\mathbf{A}}_{11}$ est supposée inversible, on définit\n",
    "alors le complément de Schur de ${\\mathbf{A}}_{11}$ dans\n",
    "${\\mathbf{A}}$ que l'on note\n",
    "${\\mathbf{A}}/{\\mathbf{A}}_{11}$\n",
    "$${\\mathbf{A}}/{\\mathbf{A}}_{11} = {\\mathbf{A}}_{22}-{\\mathbf{A}}_{21}{\\mathbf{A}}_{11}^{-1}{\\mathbf{A}}_{12}$$\n",
    "\n",
    "\n",
    "**Proposition 4.2** (Formule de diagonalisation par blocs de Aitken).\n",
    "*$$\\begin{aligned}\n",
    "    \\begin{pmatrix} {\\mathbf{I}} & {\\mathbf{0}} \\\\ -{\\mathbf{A}}_{21}{\\mathbf{A}}_{11}^{-1} & {\\mathbf{I}}\n",
    "    \\end{pmatrix}\n",
    "    \\begin{pmatrix}\n",
    "    {\\mathbf{A}}_{11}&{\\mathbf{A}}_{12}\\\\{\\mathbf{A}}_{21}&{\\mathbf{A}}_{22}\n",
    "    \\end{pmatrix}\n",
    "    \\begin{pmatrix}\n",
    "    {\\mathbf{I}}& -{\\mathbf{A}}_{11}^{-1}{\\mathbf{A}}_{12}\\\\ {\\mathbf{0}}&{\\mathbf{I}}\n",
    "    \\end{pmatrix}&=\n",
    "    \\begin{pmatrix}{\\mathbf{A}}_{11} & {\\mathbf{0}}\\\\{\\mathbf{0}} &{\\mathbf{A}}/{\\mathbf{A}}_{11}\n",
    "    \\end{pmatrix}\n",
    "    \\end{aligned}$$*\n",
    "\n",
    "**Corollaire 4.3**.\n",
    "*$\\det({\\mathbf{A}})=\\det({\\mathbf{A_{11}}})\\det({\\mathbf{A}}/{\\mathbf{A_{11}}})$*\n",
    "\n",
    "*${\\operatorname{rg}({\\mathbf{{\\mathbf{A}}}})}={\\operatorname{rg}({\\mathbf{{\\mathbf{A_{11}}}}})}+{\\operatorname{rg}({\\mathbf{{\\mathbf{A}}/{\\mathbf{A_{11}}}}})}$*\n",
    "\n",
    "*${\\operatorname{rg}({\\mathbf{{\\mathbf{A}}}})}={\\operatorname{rg}({\\mathbf{{\\mathbf{A_{11}}}}})}\\quad\\Rightarrow\\quad {\\mathbf{A}}/{\\mathbf{A_{11}}}=0$*\n",
    "\n",
    "*$\\dim({\\operatorname{Ker}({\\mathbf{A}})})=\\dim({\\operatorname{Ker}({\\mathbf{{\\mathbf{A}}/{\\mathbf{A_{11}}}}})})$*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419dd10e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Si ${\\mathbf{A}}/{\\mathbf{A_{11}}}$ est inversible\n",
    "on a également : $$\\begin{aligned}\n",
    "\\begin{pmatrix} {\\mathbf{A}}_{11}&{\\mathbf{A}}_{12}\\\\{\\mathbf{A}}_{21}&{\\mathbf{A}}_{22}\n",
    "\\end{pmatrix}&=\n",
    "\\begin{pmatrix}\n",
    "{\\mathbf{I}} & {\\mathbf{0}} \\\\ {\\mathbf{A}}_{21}{\\mathbf{A}}_{11}^{-1} & {\\mathbf{I}}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "{\\mathbf{A}}_{11} & {\\mathbf{0}}\\\\{\\mathbf{0}} &{\\mathbf{A}}/{\\mathbf{A}}_{11}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "{\\mathbf{I}}& {\\mathbf{A}}_{11}^{-1}{\\mathbf{A}}_{12}\\\\ {\\mathbf{0}}&{\\mathbf{I}}\n",
    "\\end{pmatrix}\\\\\n",
    "\\begin{pmatrix} {\\mathbf{A}}_{11}&{\\mathbf{A}}_{12}\\\\{\\mathbf{A}}_{21}&{\\mathbf{A}}_{22}\n",
    "\\end{pmatrix}^{-1}&=\n",
    "\\begin{pmatrix}\n",
    "{\\mathbf{I}}& -{\\mathbf{A}}_{11}^{-1}{\\mathbf{A}}_{12}\\\\ {\\mathbf{0}}&{\\mathbf{I}}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "{\\mathbf{A}}_{11}^{-1} & {\\mathbf{0}}\\\\{\\mathbf{0}} &({\\mathbf{A}}/{\\mathbf{A}}_{11})^{-1}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "{\\mathbf{I}} & {\\mathbf{0}} \\\\ -{\\mathbf{A}}_{21}{\\mathbf{A}}_{11}^{-1} & {\\mathbf{I}}\n",
    "\\end{pmatrix}\\\\\n",
    "&=\\begin{pmatrix}\n",
    "{\\mathbf{A}}_{11}^{-1}+{\\mathbf{A}}_{11}^{-1}{\\mathbf{A}}_{12}  ({\\mathbf{A}}/{\\mathbf{A}}_{11})^{-1} {\\mathbf{A}}_{21} {\\mathbf{A}}_{11}^{-1}&-{\\mathbf{A}}_{11}^{-1}{\\mathbf{A}}_{12} ({\\mathbf{A}}/{\\mathbf{A}}_{11})^{-1}\\\\\n",
    "-({\\mathbf{A}}/{\\mathbf{A}}_{11})^{-1}{\\mathbf{A}}_{21}{\\mathbf{A}}_{11}^{-1}&({\\mathbf{A}}/{\\mathbf{A}}_{11})^{-1}\n",
    "\\end{pmatrix}\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "**Proposition 4.4**.\n",
    "*$$({\\mathbf{A}}/{\\mathbf{A}}_{11})^{-1}=({\\mathbf{A}}^{-1})_{22}$$*\n",
    "\n",
    "Si on suppose de plus que ${\\mathbf{A}}_{22}$ est inversible,\n",
    "on peut calculer le complément de Schur\n",
    "${\\mathbf{A}}/{\\mathbf{A}}_{22}$ et on a\n",
    "$$\\begin{aligned}\n",
    "\\begin{pmatrix} {\\mathbf{A}}_{11}&{\\mathbf{A}}_{12}\\\\{\\mathbf{A}}_{21}&{\\mathbf{A}}_{22}\n",
    "\\end{pmatrix}^{-1}\n",
    "&=\\begin{pmatrix}\n",
    "({\\mathbf{A}}/{\\mathbf{A}}_{22})^{-1}&\n",
    "-({\\mathbf{A}}/{\\mathbf{A}}_{22})^{-1}{\\mathbf{A}}_{12}{\\mathbf{A}}_{22}^{-1}\\\\\n",
    "-{\\mathbf{A}}_{22}^{-1}{\\mathbf{A}}_{21} ({\\mathbf{A}}/{\\mathbf{A}}_{22})^{-1}&\n",
    "{\\mathbf{A}}_{22}^{-1}+{\\mathbf{A}}_{22}^{-1}{\\mathbf{A}}_{21}  ({\\mathbf{A}}/{\\mathbf{A}}_{22})^{-1} {\\mathbf{A}}_{12} {\\mathbf{A}}_{22}^{-1}\n",
    "\\end{pmatrix}\n",
    "\\end{aligned}$$\n",
    "\n",
    "En égalant les deux expressions, on obtient notamment :\n",
    "$$({\\mathbf{A}}/{\\mathbf{A}}_{22})^{-1}= ({\\mathbf{A}}_{11}-{\\mathbf{A}}_{12}{\\mathbf{A}}_{22}^{-1}{\\mathbf{A}}_{21})^{-1} = {\\mathbf{A}}_{11}^{-1}+{\\mathbf{A}}_{11}^{-1}{\\mathbf{A}}_{12}  ({\\mathbf{A}}/{\\mathbf{A}}_{11})^{-1} {\\mathbf{A}}_{21} {\\mathbf{A}}_{11}^{-1}$$\n",
    "qui est exactement la formule de Woodbury (Sherman, Morrisson, Duncan,\n",
    "Banachiewicz)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f26ab8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Proposition 4.5** (Formule du quotient). *Soit\n",
    "$${\\mathbf{A}}=\n",
    "    \\begin{pmatrix}\n",
    "    {\\mathbf{A}}_{11} & {\\mathbf{A}}_{12} & {\\mathbf{A}}_{13} \\\\\n",
    "    {\\mathbf{A}}_{21} & {\\mathbf{A}}_{22} & {\\mathbf{A}}_{23} \\\\\n",
    "    {\\mathbf{A}}_{31} & {\\mathbf{A}}_{32} & {\\mathbf{A}}_{33} \\end{pmatrix} = \n",
    "    \\begin{pmatrix}\n",
    "    {\\mathbf{\\tilde{A}}}_{11} &  {\\mathbf{\\tilde{A}}}_{13} \\\\\n",
    "    {\\mathbf{\\tilde{A}}}_{31} &  {\\mathbf{A}}_{33} \\end{pmatrix} \n",
    "    \\text{ avec } \n",
    "    {\\mathbf{\\tilde{A}}}_{11}=\n",
    "    \\begin{pmatrix}\n",
    "    {\\mathbf{A}}_{11} & {\\mathbf{A}}_{12} \\\\\n",
    "    {\\mathbf{A}}_{21} & {\\mathbf{A}}_{22} \\end{pmatrix},\\ \n",
    "    {\\mathbf{\\tilde{A}}}_{13}=\n",
    "    \\begin{pmatrix}\n",
    "    {\\mathbf{A}}_{13}  \\\\\n",
    "    {\\mathbf{A}}_{23}  \\end{pmatrix}$$ alors\n",
    "$${\\mathbf{A}}/{\\mathbf{\\tilde{A}}}_{11} = ({\\mathbf{A}}/{\\mathbf{A}}_{11}) / ({\\mathbf{\\tilde{A}}}_{11}/{\\mathbf{A}}_{11})$$*\n",
    "\n",
    "Cette formule est fondamentale dans la compression de l'enchaînement des\n",
    "calculs dans une élimination de Gauss : éliminer une colonne puis une\n",
    "autre est équivalente à éliminer deux colonnes d'un coup, d'où la\n",
    "possibilité de calculer un complément de Schur en bloquant un processus\n",
    "de Gauss (après numérotation convenable des degrés de libertés). Elle\n",
    "permet aussi de comprendre les méthodes de recondensation dans les\n",
    "décompositions de domaine (FETI-DP, BDDC).\n",
    "\n",
    "Profitons de l'occasion pour rajouter deux propriétés sur les\n",
    "compléments de Schur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bba993",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Proposition 4.6** (Héritage de propriétés). *Le complément de Schur\n",
    "${\\mathbf{A}}/{\\mathbf{A}}_{11}$ hérite des\n",
    "propriétés suivantes de ${\\mathbf{A}}$ :*\n",
    "*Symétrie, anti-symétrie, caractère hermitien, caractère\n",
    "anti-hermitien,*\n",
    "*Profil diagonal, triangulaire, Hessenberg, tridiagonal*\n",
    "*Caractère défini positif, *Dominance de la diagonale (par ligne et/ou par colonne), H-matrices (matrices rendues à diagonale dominante par le produit à gauche ou à\n",
    "droite par une matrice diagonale)*,*M-matrices (matrices dont les coefficients diagonaux sont strictement positifs et les autres coefficients négatifs ou nuls)*\n",
    "\n",
    "\n",
    "**Proposition 4.7** (Formule de l'inertie de Haynsworth). *Pour une\n",
    "matrice hermitienne ${\\mathbf{A}}$, on appelle inertie le\n",
    "triplet de naturels $\\operatorname{In}({\\mathbf{A}})=(p,z,n)$\n",
    "donnant le nombre de valeurs propres positives $(p)$, égales à zéro\n",
    "$(z)$ et négatives $(n)$.\n",
    "$$\\operatorname{In}({\\mathbf{A}})=\\operatorname{In}({\\mathbf{A}}_{11})+\\operatorname{In}({\\mathbf{A}}/{\\mathbf{A}}_{11})$$*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea7c494",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Notion de pseudo-inverse\n",
    "\n",
    "Pour une matrice ${\\mathbf{A}}$ donnée (rectangulaire ou\n",
    "carrée), une pseudo-inverse (ou inverse généralisée) est une matrice\n",
    "${\\mathbf{A}}^+$ qui vérifie :\n",
    "$$\\forall {\\mathbf{y}}\\in {\\operatorname{Im}({\\mathbf{A}})}, {\\mathbf{A}}{\\mathbf{A}}^+ {\\mathbf{y}} = {\\mathbf{y}}$$\n",
    "\n",
    "Bien sûr si ${\\mathbf{A}}$ est inversible, la pseudo-inverse\n",
    "coïncide avec l'inverse. Sinon, la pseudo-inverse n'est pas unique. Si\n",
    "la matrice ${\\mathbf{A}}$ est de rang $r$ alors il existe une\n",
    "matrice ${\\mathbf{A}}_{11}$ $r\\times r$ inversible et deux\n",
    "changements de bases ${\\mathbf{Q}}$ et\n",
    "${\\mathbf{P}}$ tels que $$\\begin{aligned}\n",
    "{\\mathbf{A}}&={\\mathbf{P}}\\begin{pmatrix}{\\mathbf{A}}_{11}&0\\\\0&0\\end{pmatrix}{\\mathbf{Q}}\\\\\n",
    "{\\mathbf{A}}^+&={\\mathbf{Q}}^{-1}\\begin{pmatrix}{\\mathbf{A}}_{11}^{-1}&{\\mathbf{X}}\\\\{\\mathbf{Y}}&{\\mathbf{Z}}\\end{pmatrix}{\\mathbf{P}}^{-1}\\text{  où ${\\mathbf{X}},{\\mathbf{Y}},{\\mathbf{Z}}$ sont quelconques}\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "**Proposition 4.8**. *$$\\begin{aligned}\n",
    "    {\\mathbf{A}}&={\\mathbf{A}}({\\mathbf{A}}^H{\\mathbf{A}})^+({\\mathbf{A}}^H{\\mathbf{A}})\\\\\n",
    "    {\\mathbf{A}}^H&=({\\mathbf{A}}^H{\\mathbf{A}})({\\mathbf{A}}^H{\\mathbf{A}})^+{\\mathbf{A}}^H\n",
    "    \\end{aligned}$$ Ces formules montrent que la pseudo-inversion peut\n",
    "se ramener à un problème carré. En particulier, quand\n",
    "${\\mathbf{A}}$ est de rang plein alors les matrices carrées à\n",
    "pseudo-inverser sont inversibles au sens usuel.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f725c3e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "*Proof.* On a $\\forall\\alpha$,\n",
    "$({\\mathbf{A}}^H{\\mathbf{A}})\\alpha=({\\mathbf{A}}^H{\\mathbf{A}})({\\mathbf{A}}^H{\\mathbf{A}})^+({\\mathbf{A}}^H{\\mathbf{A}})\\alpha$\n",
    "donc\n",
    "${\\mathbf{A}}^H( {\\mathbf{A}}-{\\mathbf{A}}({\\mathbf{A}}^H{\\mathbf{A}})^+({\\mathbf{A}}^H{\\mathbf{A}}))\\alpha=0$.\n",
    "Comme\n",
    "${\\operatorname{Im}({\\mathbf{A}})}\\perp{\\operatorname{Ker}({\\mathbf{A^H}})}$\n",
    "on a nécessairement\n",
    "${\\mathbf{A}}-{\\mathbf{A}}({\\mathbf{A}}^H{\\mathbf{A}})^+({\\mathbf{A}}^H{\\mathbf{A}})=0$ ◻\n",
    "\n",
    "Les pseudo-inverses peuvent vérifier certaines propriétés\n",
    "additionnelles, la pseudo-inverse la plus riche (et la moins calculable)\n",
    "est celle de Moore-Penrose ${\\mathbf{A}}^+_{MP}$, elle est la\n",
    "seule à vérifier : $$\\begin{aligned}\n",
    "{\\mathbf{A}}{\\mathbf{A}}^+_{MP}{\\mathbf{A}}={\\mathbf{A}} &\\qquad &({\\mathbf{A}}{\\mathbf{A}}^+_{MP})^H = {\\mathbf{A}}{\\mathbf{A}}^+_{MP} \\\\\n",
    "{\\mathbf{A}}^+_{MP}{\\mathbf{A}}{\\mathbf{A}}^+_{MP}={\\mathbf{A}}^+_{MP} &\\qquad & ({\\mathbf{A}}^+_{MP}{\\mathbf{A}})^H = {\\mathbf{A}}^+_{MP}{\\mathbf{A}}\n",
    "\\end{aligned}$$ La pseudo inverse de Moore-Penrose peut se calculer via\n",
    "une SVD (qui est un cas particulier de changement de base) :\n",
    "$$\\begin{aligned}\n",
    "{\\mathbf{M}}&={\\mathbf{U}}\\begin{pmatrix}{\\mathbf{\\Sigma}}&0\\\\0&0\\end{pmatrix}{\\mathbf{V}}^H\\\\\n",
    "{\\mathbf{M}}^+_{MP}&={\\mathbf{V}}\\begin{pmatrix}{\\mathbf{\\Sigma}}^{-1}&0\\\\0&0\\end{pmatrix}{\\mathbf{U}}^{H}\n",
    "\\end{aligned}$$ Comme ${\\mathbf{U}}$ et\n",
    "${\\mathbf{V}}$ sont unitaires, elles ne modifient pas les\n",
    "normes euclidiennes. On en déduit donc que la pseudo-inverse de\n",
    "Moore-Penrose donne la solution de norme minimale du problème aux\n",
    "moindres carrés associé à ${\\mathbf{A}}$.\n",
    "\n",
    "Un cas particulier important est celui des matrices unitaires\n",
    "(orthogonales) puisque l'on a directement\n",
    "${\\mathbf{U}}^+_{MP}={\\mathbf{U}}^H$. Cela implique pour une factorisation\n",
    "${\\mathbf{A}}={\\mathbf{Q}}{\\mathbf{R}}$,\n",
    "${\\mathbf{A}}^+_{MP}={\\mathbf{R}}^+_{MP}{\\mathbf{Q}}^H$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45c3fd1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Formules de calcul\n",
    "\n",
    "Une façon de trouver une pseudo-inverse s'appuie sur la connaissance des\n",
    "noyaux pour définir un problème régularisé.\n",
    "\n",
    "\n",
    "**Proposition 4.9**. *On a pour une matrice ${\\mathbf{C}}$\n",
    "quelconque $$\\begin{pmatrix}\n",
    "    {\\mathbf{A}}& {\\mathbf{N}}_g \\\\\n",
    "    {\\mathbf{N}}_d^H & {\\mathbf{C}}\n",
    "    \\end{pmatrix}^{-1} = \\begin{pmatrix}\n",
    "    {\\mathbf{A}}^+ & ? \\\\\n",
    "    ? & ?\n",
    "    \\end{pmatrix}$$*\n",
    "\n",
    "\n",
    "**Proposition 4.10**. *Si de plus ${\\mathbf{C}}$ est\n",
    "inversible, on peut calculer le complément de Schur (en posant\n",
    "${\\mathbf{C}}\\leftarrow -{\\mathbf{C}}^{-1}$ dans\n",
    "la formule précédente), on obtient alors :\n",
    "$$\\left({\\mathbf{A}}+{\\mathbf{N}}_g {\\mathbf{C}} {\\mathbf{N}}_d^H \\right)^{-1} = {\\mathbf{A}}^+$$*\n",
    "\n",
    "La pseudo-inverse de Moore-Penrose est un cas particulier de la formule\n",
    "précédente (pour ${\\mathbf{C}}=0$) :\n",
    "\n",
    "\n",
    "**Proposition 4.11**. *On a $$\\begin{pmatrix}\n",
    "    {\\mathbf{A}}& {\\mathbf{N}}_g \\\\\n",
    "    {\\mathbf{N}}_d^H & 0\n",
    "    \\end{pmatrix}^{-1} = \\begin{pmatrix}\n",
    "    {\\mathbf{A}}^+_{MP}& ({\\mathbf{N}}_d^H)^+_{MP}\\\\\n",
    "    ({\\mathbf{N}}_g)^+_{MP} & 0\n",
    "    \\end{pmatrix}$$ où l'on voit l'intérêt d'utiliser des bases\n",
    "orthonormales : dans ce cas\n",
    "$({\\mathbf{N}}_g)^+_{MP}={\\mathbf{N}}_g^H$ et\n",
    "$({\\mathbf{N}}_d^H)^+_{MP}={\\mathbf{N}}_d$*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c646be8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Calcul par blocs\n",
    "\n",
    "Une autre technique repose sur une permutation préalable.\n",
    "\n",
    "\n",
    "**Proposition 4.12**. *Si on découpe ${\\mathbf{A}}$ par blocs\n",
    "$$\\begin{pmatrix}\n",
    "    {\\mathbf{A}}_{11} & {\\mathbf{A}}_{12} \\\\ {\\mathbf{A}}_{21} & {\\mathbf{A}}_{22} \n",
    "    \\end{pmatrix}\\qquad \\text{avec }{\\mathbf{A}}_{11}\\text{ inversible et }{\\operatorname{rg}({\\mathbf{A}})}={\\operatorname{rg}({\\mathbf{A_{11}}})}$$\n",
    "On a alors $$\\label{eq:blocspinv}\n",
    "    \\begin{aligned}\n",
    "    {\\mathbf{A}}/{\\mathbf{A}}_{11}&={\\mathbf{A}}_{22} - {\\mathbf{A}}_{21}{\\mathbf{A}}_{11}^{-1}{\\mathbf{A}}_{12}={\\mathbf{0}} \\qquad\\text{(voir la notion de complément de Schur)}\\\\\n",
    "    {\\mathbf{N}}_d&=\\begin{pmatrix}\n",
    "    -{\\mathbf{A}}_{11}^{-1}{\\mathbf{A}}_{12} \\\\ {\\mathbf{I}}_{22}\n",
    "    \\end{pmatrix},\\quad{\\operatorname{Im}({\\mathbf{A}})} =\\operatorname{Im}\\begin{pmatrix}\n",
    "    {\\mathbf{I}}_{11} \\\\ {\\mathbf{A}}_{21}{\\mathbf{A}}_{11}^{-1}\n",
    "    \\end{pmatrix}\\quad\\text{par exemple} \\\\\n",
    "    {\\mathbf{A}}^+ &= \\begin{pmatrix}{\\mathbf{A}}_{11}^{-1}&0\\\\0&0\\end{pmatrix},\\quad \\text{par exemple}\n",
    "    \\end{aligned}$$*\n",
    "\n",
    "Dans le cas où ${\\mathbf{A}}$ est une matrice carrée, alors\n",
    "la pseudo inverse précédente peut être approchée par pénalisation :\n",
    "$${\\mathbf{A}}^+ = \\begin{pmatrix}{\\mathbf{A}}_{11}&{\\mathbf{A}}_{12}\\\\{\\mathbf{A}}_{21}&{\\mathbf{A}}_{22}+\\alpha{\\mathbf{I}}\\end{pmatrix}^{-1}\\ \\text{avec }\\alpha\\text{ grand},$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5243316b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Cette méthode a l'avantage d'être très peu invasive (il suffit d'accéder\n",
    "à quelques coefficients diagonaux d'une matrice creuse) et de rendre\n",
    "inversible la matrice à pseudo-inverser (qui peut donc utiliser une\n",
    "factorisation classique), le grand inconvénient de cette méthode est de\n",
    "détériorer fortement le conditionnement de l'opérateur.\n",
    "\n",
    "D'après la proposition précédente, la connaissance d'une permutation\n",
    "(*ie* l'organisation en blocs avec le bloc $(1,1)$ inversible de rang\n",
    "égal à celui de la matrice) permet de connaître les noyaux (et donc\n",
    "potentiellement d'employer aussi les premières formules).\n",
    "Réciproquement, la connaissance des noyaux permet de trouver une\n",
    "permutation qui isole une matrice carrée de rang égal à celui de la\n",
    "matrice complète. En effet on voit que le noyau est de rang plein sur les noeuds\n",
    "de type $2$ ; cette propriété est résistante aux changements de base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf9af20",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On raisonne dans le cas symétrique (sinon il faut distinguer permutation\n",
    "à gauche et à droite). Pour le noyau ${\\mathbf{N}}$ donné, si\n",
    "la matrice carrée obtenue par extraction des lignes\n",
    "$(i_1,...i_{\\dim({\\operatorname{Ker}_d({\\mathbf{A}})})})$\n",
    "de ${\\mathbf{N}}$ est inversible alors ces lignes peuvent\n",
    "être choisies pour définir le bloc $2$ de la permutation. Mécaniquement\n",
    "cela correspond à bloquer des degrés de liberté de manière à supprimer\n",
    "les modes rigides.\n",
    "\n",
    "On propose la technique suivante :\n",
    "\n",
    "-   on prend $p_1={\\operatorname{argmax}}_i(|n_{i1}|)$ degré\n",
    "    de liberté où le premier déplacement rigidifiant est le plus grand\n",
    "\n",
    "-   pour\n",
    "    $j=2\\cdots \\dim({\\operatorname{Ker}({\\mathbf{A}})})$\n",
    "    on met à jour\n",
    "    ${\\mathbf{n}}_j\\leftarrow {\\mathbf{n}}_j-\\frac{n_{p_1 j}}{n_{p_1 1}}{\\mathbf{n}}_1$\n",
    "    de manière à annuler la composante $p_1$ des autres déplacements\n",
    "    rigides,\n",
    "\n",
    "-   on prend $p_2={\\operatorname{argmax}}_i(|n_{i2}|)$\n",
    "    puisqu'on est sûr qu'il bloque le deuxième déplacement rigidifiant,\n",
    "\n",
    "-   on met à jour pour $j=3\\cdots z$\n",
    "    ,${\\mathbf{n}}_j\\leftarrow {\\mathbf{n}}_j-\\frac{n_{p_2 j}}{n_{p_2 2}}{\\mathbf{n}}_2$\n",
    "    de manière à annuler la composante $p_2$ des autres déplacements\n",
    "    rigides,\n",
    "\n",
    "-   on itère jusqu'à l'avant dernière colonne du noyau.\n",
    "\n",
    "Cette méthode correspond à choisir des degrés de liberté éloignés pour\n",
    "bloquer la structure ce qui conduit à un meilleur conditionnement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a594becf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Obtention des informations relatives à la déficience de rang\n",
    "\n",
    "On a vu qu'il était nécessaire de connaître soit une base du noyau soit\n",
    "une renumérotation permettant d'isoler un bloc de rang plein (et que ces\n",
    "deux connaissances étaient équivalentes). On donne ici des techniques\n",
    "qui permettent d'obtenir une de ces deux informations. On ne parle pas\n",
    "ici des factorisations QR, qui donnent toutes les informations\n",
    "souhaitables mais sont souvent trop lentes.\n",
    "\n",
    "On a vu qu'il était possible de ramener un problème à un système carré\n",
    "et on s'intéresse donc à ce cas là (ceci dit ce n'est pas\n",
    "indispensable)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169af210",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Méthode algébrique\n",
    "\n",
    "Si on s'intéresse à la factorisation de Crout (ou $LDU$) d'une matrice,\n",
    "les singularités se manifestent par la présence de pivots nuls. Bien sûr\n",
    "la nullité numérique peut se révéler très difficile à définir ; on\n",
    "compare en général les pivots entre eux et si le conditionnement est\n",
    "mauvais l'écart entre les pivots peut être très faible. Il faut au moins\n",
    "utiliser un pivot partiel ; un pivot total est très sécurisant dans ce\n",
    "cas là.\n",
    "\n",
    "Quand un pivot nul est détecté, une façon relativement peu invasive de\n",
    "travailler est d'annuler les coefficients de la ligne et la colonne du\n",
    "pivot et de mettre un $1$ sur la diagonale du pivot. On peut alors finir\n",
    "la factorisation, il suffit ensuite de penser à annuler les composantes\n",
    "du second membre associées aux pivots nuls avant la descente/remontée.\n",
    "Une idée intéressante est de stocker ${\\mathbf{D}}^{-1}$ (au\n",
    "lieu de ${\\mathbf{D}}$) en mettant des $0$ sur les pivots\n",
    "singuliers.\n",
    "\n",
    "Une autre technique consiste à réaliser une pénalisation des pivots nuls\n",
    ": on remplace le pivot nul par un pivot énorme. Numériquement cette\n",
    "méthode est bien sûr moins propre).\n",
    "\n",
    "Il faut remarquer qu'en pratique sauf numérotation et géométrie\n",
    "étranges, les pivots nuls se retrouvent à la fin de la matrice car les\n",
    "déplacements de solide rigide sont globaux sur toute la structure et\n",
    "c'est après avoir fait pivoter pratiquement tous les degrés de liberté\n",
    "qu'on arrive à annuler une colonne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e944fecf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Méthode mécanique\n",
    "\n",
    "On possède souvent une information sur les noyaux car ils sont issus de\n",
    "la physique, de la discrétisation, de la linéarisation. En mécanique on\n",
    "peut exploiter le fait que l'on sait *a priori* d'où provient le noyau.\n",
    "U\n",
    "\n",
    "Une première technique consiste à utiliser une base candidate du noyau\n",
    "construite à partir des déplacements rigides de la structure supposée\n",
    "libre (trois translations, trois rotations infinitésimales). Cette base\n",
    "peut-être construite dès que l'on connaît la position des nœuds. On\n",
    "évalue alors cette base $\\tilde{{\\mathbf{N}}}$ ($n\\times 6$\n",
    "en 3D) en calculant\n",
    "$\\tilde{{\\mathbf{M}}}={\\mathbf{A}}\\tilde{{\\mathbf{N}}}$,\n",
    "il faut alors calculer le rang et une base de l'image de\n",
    "$\\tilde{{\\mathbf{M}}}$. Soit une matrice $R$ ($6\\times 6$),\n",
    "obtenue par orthogonalisation par exemple, telle que\n",
    "$$\\tilde{{\\mathbf{M}}}{\\mathbf{R}}=\\begin{pmatrix} {\\mathbf{M}} & {\\mathbf{0}}\\end{pmatrix},\\quad {\\operatorname{rg}({\\mathbf{M}})}={\\operatorname{rg}({\\mathbf{\\tilde{M}}})}$$\n",
    "alors les $(6-{\\operatorname{rg}({\\mathbf{M}})})$\n",
    "dernières colonnes de la matrice\n",
    "$\\tilde{{\\mathbf{N}}}{\\mathbf{R}}$ forment une\n",
    "base du noyau de ${\\mathbf{A}}$. Cette technique repose\n",
    "toujours sur l'évaluation d'un zéro numérique pour calculer le rang de\n",
    "$\\tilde{{\\mathbf{M}}}$.\n",
    "\n",
    "Une technique plus sophistiquée consiste à analyser directement les\n",
    "conditions aux limites de Dirichlet. Pour cela on construit la matrice\n",
    "${\\mathbf{C}}$ $(n \\times d)$ dont chaque colonne de\n",
    "représente un degré de liberté bloqué. Alors\n",
    "${\\mathbf{W}}={\\mathbf{C}}^T\\tilde{{\\mathbf{N}}}$\n",
    "représente le travail de les déplacements de solide rigide candidat dans\n",
    "les conditions aux limites imposées. Les déplacements réellement rigides\n",
    "sont les $\\tilde{{\\mathbf{N}}}{\\mathbf{\\beta}}$\n",
    "combinaisons linéaires de déplacements candidats où\n",
    "${\\mathbf{\\beta}}$ est une base du noyau de\n",
    "${\\mathbf{W}}$. Pour trouver cette base, le plus élégant et\n",
    "le plus stable est d'utiliser la SVD. L'intérêt d'une telle méthode est\n",
    "de ne pas faire intervenir la rigidité de la structure et donc d'être\n",
    "beaucoup mieux conditionnée."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3620530",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Complément de Schur généralisé\n",
    "\n",
    "La notion de complément de Schur se généralise en considérant le cas où\n",
    "la matrice dénominateur n'est pas inversible (bien qu'il soit possible,\n",
    "moyennant des précautions, de travailler sur des matrices\n",
    "rectangulaires, on considère ici ${\\mathbf{A}}$,\n",
    "${\\mathbf{A}}_{11}$ et ${\\mathbf{A}}_{22}$\n",
    "carrées).\n",
    "$${\\mathbf{A}}=\\begin{pmatrix}{\\mathbf{A}}_{11}&{\\mathbf{A}}_{12}\\\\{\\mathbf{A}}_{21}&{\\mathbf{A}}_{22}\\end{pmatrix}$$\n",
    "on définit alors le complément de Schur généralisé :\n",
    "$${\\mathbf{A}}/{\\mathbf{A}}_{11} = {\\mathbf{A}}_{22}-{\\mathbf{A}}_{21}{\\mathbf{A}}_{11}^{+}{\\mathbf{A}}_{12}$$\n",
    "\n",
    "La question fondamentale est de savoir à quelle condition le complément\n",
    "de Schur généralisé dépend du choix de l'inverse généralisée. On voit\n",
    "que la condition fondamentale est que\n",
    "${\\operatorname{Im}({\\mathbf{{A}_{12}}})}\\subset {\\operatorname{Im}({\\mathbf{{A}_{11}}})}$\n",
    "(comme ça le résultat de la multiplication\n",
    "${\\mathbf{A}}_{11}^{+}{\\mathbf{A}}_{12}$ est\n",
    "déterminé de manière unique).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc503eaf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Proposition 4.13**. *Si ${\\mathbf{A}}$ hermitienne\n",
    "semi-définie positive.*\n",
    "\n",
    "*On a nécessairement\n",
    "${\\operatorname{Im}({\\mathbf{{A}_{12}}})}\\subset {\\operatorname{Im}({\\mathbf{{A}_{11}}})}$\n",
    "et le complément de Schur généralisé est bien défini. On a alors\n",
    "$${\\operatorname{rg}({\\mathbf{A}})}={\\operatorname{rg}({\\mathbf{A_{11}}})}+{\\operatorname{rg}({\\mathbf{{A}/{A}_{11}}})}$$*\n",
    "\n",
    "*On a une formule sur les pseudo inverses de Moore-Penrose dans le cas\n",
    "où\n",
    "${\\operatorname{rg}({\\mathbf{A}})}={\\operatorname{rg}({\\mathbf{A_{11}}})}+{\\operatorname{rg}({\\mathbf{{A}_{22}}})}$\n",
    "$$\\begin{pmatrix} {\\mathbf{A}}_{11}&{\\mathbf{A}}_{12}\\\\{\\mathbf{A}}_{21}&{\\mathbf{A}}_{22}\n",
    "    \\end{pmatrix}^{+}_{MP}\n",
    "    =\\begin{pmatrix}\n",
    "    {{\\mathbf{A}}_{11}}^{+}_{MP}+({\\mathbf{A}}_{11})^{+}_{MP}{\\mathbf{A}}_{12}  ({\\mathbf{A}}/{\\mathbf{A}}_{11})^{+}_{MP} {\\mathbf{A}}_{21} ({\\mathbf{A}}_{11})^{+}_{MP}&-({\\mathbf{A}}_{11})^{+}_{MP}{\\mathbf{A}}_{12} ({\\mathbf{A}}/{\\mathbf{A}}_{11})^{+}_{MP}\\\\\n",
    "    -({\\mathbf{A}}/{\\mathbf{A}}_{11})^{+}_{MP}{\\mathbf{A}}_{21}({\\mathbf{A}}_{11})^{+}_{MP}&({\\mathbf{A}}/{\\mathbf{A}}_{11})^{+}_{MP}\n",
    "    \\end{pmatrix}$$*"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Aucun(e)",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

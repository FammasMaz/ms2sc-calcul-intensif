{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f1d148",
   "metadata": {},
   "source": [
    "# Projet de méthodes matricielles pour le calcul intensif\n",
    "## P.A. Boucard\n",
    "Le projet est divisé en 3 parties globalement indépendantes :\n",
    "- Comparaison de la convergence de solveurs itératifs basée sur les méthodes de stationnarité (Jacobi, Gausse-Seidel et SOR)\n",
    "- Méthode multigrille pour le problème de Poisson\n",
    "- Résolution de problèmes non-linéaires\n",
    "\n",
    "Le rendu du projet se fera sous la forme d'un notebook jupyter qui comprendra les réponses aux questions posées et intègrera le code Python (très commenté !). Les fichiers seront nommés **Projet_NOM_Prénom.ipynb**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e69ad69",
   "metadata": {},
   "source": [
    "## Comparaison de la convergence de solveurs itératifs\n",
    "\n",
    "### Résolution numérique de l'équation de la chaleur unidimensionnelle\n",
    "\n",
    "On cherche à résoudre l'équation $u''(x)+f(x), x \\in [0,1]$ avec $u(0)=u(1)=0$.\n",
    "\n",
    "On va pour cela introduire la méthode de discrétisation dite par différences finies. Soit $n \\in \\mathbb{N}$, on définit $h = 1/(n + 1)$ le pas de discrétisation, c.à.d. la distance entre deux points de discrétisation, et pour $ i=0,...,n+1$ on définit les points de discrétisation $x_i =ih$.\n",
    "\n",
    "En imposant $u_0=0$ en $x_0$ et $u_n=0$ en $x_n$, on peut montrer que le système à résoudre s'écrit : $\\mathbf{K}\\mathbf{u}=\\mathbf{b}$ ou $b$ dépend du choix de $f$.\n",
    "\n",
    "La matrice $\\mathbf{K}$ est tridiagonale, symétrique définie positive et telle que $$\\begin{aligned}\n",
    " K_{i,i} &= \\frac{2}{h^2}, \\forall i=1,...,n \\\\\n",
    " K_{i,j} &= \\frac{-1}{h^2}, \\forall i=1,...,n, j=i\\pm1 \\\\\n",
    " K_{i,j} &= 0, \\forall i=1,...,n, |i-j|\\gt 1\n",
    "\\end{aligned}$$\n",
    "    \n",
    "Dans la suite on choisit $f(x)=\\pi^2 sin(\\pi x)$, la solution exacte du problème est donc $u(x)=sin(\\pi x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194a047c",
   "metadata": {},
   "source": [
    "#### Question :\n",
    "*Construire la matrice $\\mathbf{K}$, et le second membre $\\mathbf{b}$, ainsi que la solution exacte pour $n$ quelconque. En utilisant le solveur directe de numpy, montrer numériquement la convergence d’ordre 2 pour la norme $\\|\\;\\;\\|_{\\infty}$ (on pourra tracer le logarithme de la norme infinie entre la solution approchée et la solution exacte en fonction du logarithme de $h$, et calculer la pente de la droite qui en résulte).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "309d43b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d5dee9",
   "metadata": {},
   "source": [
    "### Comparaison de solveurs itératifs\n",
    "\n",
    "On propose de reprendre le méthodes de stationnarité qui reposent sur le partitionnement de la matrice\n",
    "${\\mathbf{K}}={\\mathbf{M}}-{\\mathbf{N}}$\n",
    "avec ${\\mathbf{M}}$ facile à inverser. On réécrit le système : ${\\mathbf{M}}{\\mathbf{x}}= {\\mathbf{N}}{\\mathbf{x}}+{\\mathbf{b}}$\n",
    "\n",
    "On utilise un schéma de type point fixe : ${\\mathbf{M}}{\\mathbf{x}}_{i+1} = {\\mathbf{N}}{\\mathbf{x}}_i+{\\mathbf{b}}$ qui converge dès que $\\rho({\\mathbf{M}}^{-1}{\\mathbf{N}})<1$.\n",
    "\n",
    "Le principal critère est lié au résidu, on itère tant que $\\|{\\mathbf{r}}_i\\|>\\varepsilon\\|{\\mathbf{b}}\\|$\n",
    "\n",
    "On écrit ${\\mathbf{K}}={\\mathbf{D}}-{\\mathbf{E}}-{\\mathbf{F}}$\n",
    "avec ${\\mathbf{D}}$ diagonale ${\\mathbf{E}}$\n",
    "strictement triangulaire inférieure et ${\\mathbf{F}}$\n",
    "strictement triangulaire supérieure. On va alors étudier les 3 méthodes suivantes :\n",
    "\n",
    "-   Jacobi : ${\\mathbf{M}}={\\mathbf{D}}$ et\n",
    "    ${\\mathbf{N}}={\\mathbf{E}}+{\\mathbf{F}}$\n",
    "    ,\n",
    "\n",
    "-   Gauss-Seidel :\n",
    "    ${\\mathbf{M}}={\\mathbf{D}}-{\\mathbf{E}}$\n",
    "    et ${\\mathbf{N}}={\\mathbf{F}}$,\n",
    "    \n",
    "-   Sur-relaxation, $\\omega$ paramètre réel:\n",
    "    ${\\mathbf{M}}={\\frac{1}{\\omega}\\mathbf{D}}-\\mathbf{E}$\n",
    "    et\n",
    "    ${\\mathbf{N}}=(\\frac{1}{\\omega}-1){\\mathbf{D}}+{\\mathbf{F}}$\n",
    "    (en Anglais on parle de *successive overrelaxation* (SOR)),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab964d",
   "metadata": {},
   "source": [
    "#### Question :\n",
    "*Construire une fonction dont les paramètres sont $\\mathbf{K}, \\mathbf{b}, \\epsilon, \\mathbf{x}_{ex}$ et $\\mathbf{x_0}$ et qui calcule, avec la méthode de Jacobi, la solution approchée du système linéaire $\\mathbf{K}\\mathbf{x}=\\mathbf{b}$ et retourne le résidu ($\\|{\\mathbf{r}}_i\\|$) et l’erreur ($\\|{\\mathbf{x}} - {\\mathbf{x}}_i\\|$) à chaque itération $i$ de la méthode (${\\mathbf{x}}$ sera obtenu par une méthode directe).*\n",
    "\n",
    "*Vérifier pour le problème associé à la discrétisation de l'équation de la chaleur (on prendra $n=50$) que $\\rho({\\mathbf{M}}^{-1}{\\mathbf{N}})<1$.*\n",
    "\n",
    "*Construire pour $\\epsilon = 10^{−6}$ et deux choix différents de $\\mathbf{x_0}$ (par exemple, $\\mathbf{x_0}$ nul et $\\mathbf{x_0}$ dont toutes les composantes sont égales à 1) un graphique, avec une échelle semilog, donnant l'évolution de l’erreur au cours des itérations.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82802371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "038b67bd",
   "metadata": {},
   "source": [
    "#### Question :\n",
    "\n",
    "*Reprendre la question précédente pour la méthode de Gauss-Seidel*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99c970d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21b4d17a",
   "metadata": {},
   "source": [
    "#### Question :\n",
    "\n",
    "*On s’intéresse maintenant à la méthode SOR : ${\\mathbf{M}}={\\frac{1}{\\omega}\\mathbf{D}}-\\mathbf{E}$\n",
    "    et\n",
    "    ${\\mathbf{N}}=(\\frac{1}{\\omega}-1){\\mathbf{D}}+{\\mathbf{F}}$*\n",
    "\n",
    "*On suppose que $0 \\lt \\omega \\lt 2$*\n",
    "\n",
    "*Déterminer numériquement une approximation de la valeur de $\\omega$ minimisant $\\rho({\\mathbf{M}}^{-1}{\\mathbf{N}})$.*\n",
    "\n",
    "*Reprendre la question précédente avec la valeur de $\\omega$ déterminée.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538ffb90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfb7122e",
   "metadata": {},
   "source": [
    "#### Question :\n",
    "\n",
    "*On concluera en étudiant l'influence de l'initialisation, en comparant la convergence des différentes stratégies, etc.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc6ea89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "579ab763",
   "metadata": {},
   "source": [
    "##  Méthode multigrille pour le problème de Poisson\n",
    "\n",
    "On cherche toujours à résoudre l'équation $u''(x)+f(x), x \\in [0,1]$ avec $u(0)=u(1)=0$ qui est le problème de Poisson.\n",
    "\n",
    "On va résoudre cette équation avec une méthode multigrille. \n",
    "\n",
    "    L’idée principale de la méthode multigrille est que certaines itérations classiques, telles par exemple les itérations de Jacobi relaxées agissent comme des filtres passe-bas sur la solution : elles amortissent beaucoup les hautes fréquences, alors que les basses fréquences changent peu en une itération ; donc, si on veut réduire efficacement le résidu, après l’avoir débarassé de ses hautes fréquences, il faut faire une correction sur une grille plus grossière, ayant par exemple un pas d’espace deux fois plus grand.\n",
    "\n",
    "Pour expliquer le principe de la méthode multigrille, on la méthode à 2 grilles ; cette dernière combine deux méthodes peu performantes pour finalement obtenir une méthode très efficace. On considère une grille fine de pas $h$ et une grille grossière de pas $H=2h$ pour résoudre le problème.\n",
    "\n",
    "\n",
    "Un cycle de la méthode 2-Grilles se compose de deux phases :\n",
    "\n",
    "- une méthode de lissage, correspondant à deux ou trois itérations d'une méthode de relaxation (par exemple SOR avec $\\omega=1$ qui revient à la méthode de Gauss-Seidel), permettant de réduire les hautes fréquences de l'erreur lorsque l'on décompose celle-ci dans la base des vecteurs propres de la matrice ; soit $\\mathbf{u}_{h}$ l'approximation de $\\mathbf{U}_{h}$ solution du système linéaire discret  $\\mathbf{A}_{h} \\mathbf{U}_{h}=\\mathbf{b}_{h}$\n",
    "- une méthode de correction sur grille grossière qui traite efficacement les basses fréquences de l'erreur . La correction  $\\mathbf{v}_{h}=\\mathbf{U}_{h}-\\mathbf{u}_{h}$ , qu'il faut ajouter à $\\mathbf{u}_{h}$ (approximation) pour obtenir $\\mathbf{U}_{h}$ (solution exacte), doit vérifier :$\\mathbf{A} \\mathbf{v}_{h}=\\mathbf{r}_{h}$ avec $\\mathbf{r}_{h}=\\mathbf{b}_{h}-\\mathbf{A} \\mathbf{u}_{h}$\n",
    "\n",
    "\n",
    "$\\mathbf{v}_{h}$ est donc la solution d'un problème du même type que celui qui définit $\\mathbf{U}_{h}$, où le second membre $\\mathbf{b}_{h}$ a été remplacé par le résidu $\\mathbf{r}_{h}$ aux points de la grille fine. Le calcul de $\\mathbf{v}_{h}$ est *a priori* aussi coûteux que celui de $\\mathbf{u}_{h}$ . Dans la mesure où l'erreur est lissée, on peut cependant chercher à obtenir une approximation de $\\mathbf{v}_{h}$ sur la grille grossière de pas $H$, ce qui nécessitera beaucoup moins de calculs si la grille de pas $H$ possède 2 fois moins de points dans le cas du problème monodimensionnel (4 fois moins, dans le cas du problème bidimensionnel, 8 fois moins dans le cas du problème tridimensionnel) que celle de pas $h$ ; on pourra effectivement obtenir une bonne approximation de $\\mathbf{v}_{h}$ sur la grille grossière si $\\mathbf{v}_{h}$ varie lentement, ce qui est effectivement le cas grâce aux itérations de lissage. La seconde phase de chaque cycle de la méthode 2-Grilles consiste alors à :\n",
    "- définir la restriction $\\mathbf{r}_{H}$ de $\\mathbf{r}_{h}$ sur la grille grossière à l'aide d'un opérateur de restriction ; cet opérateur peut être, tout simplement, l'injection définie en tous points M de la grille grossière, mais on peut également utiliser une restriction par pondération ;\n",
    "- résoudre le système linéaire $\\mathbf{A}_{H}\\mathbf{v}_{H}=\\mathbf{r}_{H}$ , donnant la correction $\\mathbf{v}_{H}$ sur la grille grossière ;\n",
    "- prolonger $\\mathbf{v}_{H}$ obtenu sur la grille grossière en une fonction $\\mathbf{v}_{h}$  obtenue sur la grille fine, par exemple par interpolation linéaire.\n",
    "- On recommence ensuite un nouveau cycle de la méthode 2-Grilles, la résolution par la méthode de relaxation se faisant en prenant comme initialisation $\\mathbf{u}_{h}$ mis à jour, soit $\\mathbf{u}_{h}$ + $\\mathbf{v}_{h}$.\n",
    "\n",
    "On peut bien sûr re-utiliser cette méthode pour résoudre le problème linéaire sur la grille grossière, qu'on va donc résoudre sur une grille encore plus grossière, etc, et on construit une approche récursive, jusqu’à une grille grossière sur laquelle une résolution directe est possible (par exemple.. 1 point). On construit alors une méthode dite \"cycle en V\". On peut également construire un \"cycle en W\", ou avec des itérations emboitées le \"Full Multigrid V-cycle\". \n",
    "\n",
    "\n",
    "Pour mettre en oeuvre la méthode à 2 grilles:\n",
    "- on définit $n_H$ point sur la grille grossière (+ les 2 points extrêmes pour les conditions aux limites qui sont éliminés de l'opérateur à \"inverser\")\n",
    "- on définit $n_h = 2 * n_H +1$ points sur la grille fine (+ les 2 points extrêmes pour les conditions aux limites qui sont éliminés de l'opérateur à \"inverser\"). Ainsi chaque point de la grille grossière est confondu avec un point de la grille fine.\n",
    "\n",
    "Pour l'opérateur d'interpolation de la grille grossière vers la grille fine on fait une simple interpolation linéaire.\n",
    "\n",
    "On écrit alors : $\\mathbf{v}_h=\\mathbf{I}_H^h \\mathbf{v}^H$, avec $\\mathbf{I}_H^h$ de taille $(n_h,n_H)$ vérifiant $$\\begin{aligned}\n",
    " \\mathbf{v}_{h_{2j}} &= \\mathbf{v}_{H_{j}} \\\\\n",
    " \\mathbf{v}_{h_{2j+1}} &= \\frac{1}{2}(\\mathbf{v}_{H_{j}}+\\mathbf{v}_{H_{j+1}}) \\\\\n",
    "  0 \\leq &j \\leq n_H\n",
    "\\end{aligned}$$\n",
    "\n",
    "Pour l'opérateur de restriction de la grille fine vers la grossière on fait une pondération (full weighting).\n",
    "\n",
    "On écrit alors : $\\mathbf{v}_H=\\mathbf{I}_h^H \\mathbf{v}_h$, avec $\\mathbf{I}_h^H$ de taille $(n_H,n_h)$ vérifiant $$\\begin{aligned}\n",
    " \\mathbf{v}_{H_{j}} &= \\frac{1}{4}\\mathbf{v}_{h_{2j-1}}+\\frac{1}{2}\\mathbf{v}_{h_{2j}}+ \\frac{1}{4}\\mathbf{v}_{h_{2j+1}} \\\\\n",
    " 1 \\leq &j \\leq n_H\n",
    "\\end{aligned}$$\n",
    "\n",
    "On vérifiera que $\\mathbf{I}_H^h = 2({\\mathbf{I}_h^H})^T$\n",
    "\n",
    "On note qu'on peut facilement calculer $\\mathbf{A}_{H} = \\mathbf{I}_h^H \\mathbf{A}_{h} \\mathbf{I}_H^h$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4036cf46",
   "metadata": {},
   "source": [
    "#### Question :\n",
    "\n",
    "*En reprenant la discrétisation par différence finie de l'équation, résoudre par une méthode à 2 grilles le problème. On utilisera pour le filtrage la méthode de Jacobi relaxée (${\\mathbf{x}}_{i+1} = \\omega {\\mathbf{M}}^{-1}({\\mathbf{N}}{\\mathbf{x}}_i+{\\mathbf{b}})+(1-\\omega)\\mathbf{x}_i$, on prendra par exemple $\\omega=\\frac{2}{3}$) avec quelques itérations (par exemple 5).*\n",
    "\n",
    "*On choisira $n_H$ élements sur la grille grossière, et donc $\\frac{1}{n_H+1}$ est le pas de la grille grossiere, le nombre d'éléments de la grille fine est $n_h = 2 n_H + 1$ (pas de $\\frac{1}{n_h+1}$).*\n",
    "\n",
    "*Construire l'opérateur de restriction de la grille fine vers la grille grossière et en déduire l'opérateur d'interpolation de grille grossiere vers grille fine.*\n",
    "\n",
    "*On utilisera ces opérateurs pour les projections entre grilles.*\n",
    "\n",
    "*Comparer le nombre d'itérations totales pour obtenir un certain niveau d'erreur avec le nombre d'itérations de la méthode Jacobi standard*\n",
    "\n",
    "***Option :*** *Programmer la méthode avec le cycle en V*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7863be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1abb0a5",
   "metadata": {},
   "source": [
    "## Résolution de problèmes non-linéaires\n",
    "\n",
    "### Illustration de l'ordre de convergence des méthodes de résolution de systèmes non linéaires en 1D\n",
    "\n",
    "#### Rappels sur la méthode de Newton\n",
    "La méthode de Newton ou méthode de Newton-Raphson est une méthode numérique itérative de résolution numérique des équations du type $f(x)=0$. Elle repose sur la méthode du point fixe avec une fonction $g$ particulière qui dépend de la dérivée de $f$ :\n",
    "\n",
    "$g(x)=x-\\displaystyle\\frac{f(x)}{f’(x)}$\n",
    "\n",
    "Rechercher un point fixe de l’application \n",
    "$g$ revient à chercher une solution de l’équation $f(x)=0$. Et la recherche du point fixe se fait via le schéma numérique suivant :\n",
    "\n",
    "$x_{n+1}=x_n-\\displaystyle\\frac{f(x_n)}{f’(x_n)}$\n",
    "\n",
    "On peut construire une variante de la méthode de Newton en calculant une sécante, ce qui revient à remplacer ${f’(x_n)}$ par $\\frac{f(x_n)-f(x_0)}{x_n-x_0}$\n",
    "\n",
    "Bien évidemment cette méthode écrite ici pour une fonction scalaire à une fonction $\\mathbf{f} : \\mathbb{R}^m \\rightarrow \\mathbb{R}^m$, et on doit écrit alors :\n",
    "\n",
    "$\\mathbf{x}_{n+1}=\\mathbf{x}_n-\\mathbf{Df}(\\mathbf{x}_n)^{-1}{\\mathbf{f}(\\mathbf{x}_n)}$\n",
    "\n",
    "où $\\mathbf{Df}(\\mathbf{x}_n)$ est la matrice jacobienne de $\\mathbf{f}$ (matrice qui a pour dimension $m$, calculée en $\\mathbf{x}=\\mathbf{x}_n$).\n",
    "\n",
    "#### Comparaison de 3 méthodes : dichotomie, Newton et de Newton-sécante.\n",
    "On utilise 3 méthodes pour la recherche d'une valeur approchée de $\\sqrt{2}$ à $10^{-15}$ près.\n",
    "\n",
    "On cherche donc le zéro de la fonction $f(x)=x^2-2$ qui a pour dérivée $f'(x)=2x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd56ee20",
   "metadata": {},
   "source": [
    "#### Question :\n",
    "\n",
    "*Programmer les 3 méthodes sous forme de fonctions, et pour un même niveau d'erreur, comparer les résultats obtenus. Ces fonctions retourneront le nombre d'itérations, la solution obtenue, et l'évolution des solutions au cours des itérations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb4b8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7bcf86d",
   "metadata": {},
   "source": [
    "#### Question :\n",
    "\n",
    "*Pour chaque algorithme, tracer un graphique permettant de retrouver l'ordre de convergence*\n",
    "\n",
    "On rappelle que pour :\n",
    "- la dichotomie : la suite d'approximation $x_{n+1}$ approchant la valeur $\\alpha$ vérifie $\\|x_{n+1} - \\alpha\\| \\lt C \\|x_{n} - \\alpha\\|^1$,la méthode est donc d'ordre 1. Donc en traçant $log(\\|x_{n+1} - \\alpha\\|)$ en fonction de $log(\\|x_{n+1} - \\alpha\\|)$, on devrait obtenir une droite de pente 1.\n",
    "- la sécante : la suite d'approximation $x_{n+1}$ approchant la valeur $\\alpha$ vérifie $\\|x_{n+1} - x_{n}\\| \\lt C \\|x_{n} - x_{n-1}\\|^\\beta$, $\\beta$ étant le nombre d'or ($\\beta=\\frac{1+\\sqrt5}{2}$).\n",
    "- Newton : la suite d'approximation $x_{n+1}$ approchant la valeur $\\alpha$ vérifie $\\|x_{n+1} - x_{n}\\| \\lt C \\|x_{n} - x_{n-1}\\|^2$, la méthode est donc d'ordre 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d4323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc324d4a",
   "metadata": {},
   "source": [
    "### Résolution d'une équation complexe\n",
    "\n",
    "On s’intéresse à la recherche des solutions complexes de l’équation $z^3 = 1$. Pour cela nous allons utiliser la méthode de Newton appliquée à la fonction vectorielle de 2 variables\n",
    "$F : (x,y)->(Re((x+i y)^3-1),Im((x+i y)^3-1))$, fonction qui s'annule exactement lorsque $(x+i y)^3=1$.\n",
    "On donne ci-dessous le code Python définissant cette fonction (que l'on compile via jit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2017db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit, float64\n",
    "\n",
    "@jit('float64[:](float64, float64)',nopython=True)\n",
    "def F(x,y):\n",
    "    return np.array([-1.+x**3-3.*x*y**2,3.*x**2*y-y**3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da23b429",
   "metadata": {},
   "source": [
    "#### Question :\n",
    "*Justifier cette implémentation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c8ceba",
   "metadata": {},
   "source": [
    "#### Question :\n",
    "Calculer les solutions de l'équation $z^3 = 1$ et vérifier en Python que la fonction vectorielle précédemment définie s'annule bien pour ces 3 valeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1184f515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23046825",
   "metadata": {},
   "source": [
    "#### Question :\n",
    "Définir une fonction DF qui retourne la différentielle de la F (sa matrice Jacobienne)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5454fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00f9d10e",
   "metadata": {},
   "source": [
    "#### Question :\n",
    "Définir une fonction Newton, qui pour une initialisation donnée et une tolérance fixée calcule et retourne les valeurs des parties réelle et imaginaire de la solution de l'équation $z^3 = 1$. On retournera également le nombre d'itérations pour atteindre la tolérance (que l'on limitera à 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f934d9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7d62905",
   "metadata": {},
   "source": [
    "On choisit maintenant l'initialisation dans une grille couvrant le carré $[−1.5, 1.5] \\times [−1.5, 1.5]$ de pas $h = 3/n$, $n$ étant un entier.\n",
    "En choisissant comme initialisation chaque point de la grille $(x_i , y_j )$, on résoud alors $F(x, y) = 0$ par la méthode de Newton et on mémorise dans un premier tableau, on mémorisera le numéro $k$ entre 1 et 3 de la racine $exp(2ik\\pi/3)$ vers lequel l’algorithme aura convergé. Dans un deuxième tableau, on mémorisera le nombre d'itération pour converger. \n",
    "#### Question :\n",
    "Programmer cette recherche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1417a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a4cd4ee",
   "metadata": {},
   "source": [
    "#### Question :\n",
    "\n",
    "*À partir des résultats précédents construire une image de taille $n \\times n$. Chaque point de cette image prend une couleur différente en fonction de la racine vers laquelle l'algorithme a convergé. La teinte de cette couleur sera pondérée par le nombre d'itérations nécessaires pour converger (le codage de couleurs HSV est adapté à la description précédente, il faudra ensuite la convertir en RGB pour créer l'image).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791094cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e28eb33e",
   "metadata": {},
   "source": [
    "#### Question :\n",
    "* Constuire une nouvelle image, mais cette fois sur le domaine de taille  $ 0.2 \\times 0.2$ centré en $[0.5,0.5]$. Que peut-on conjecturer sur la nature de cette représentation graphique ?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d67e5b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c1fe779",
   "metadata": {},
   "source": [
    "#### Question :\n",
    "\n",
    "*Reprendre la méthode précédente et construire l'image associée à la recherche des zéros du polynome $p(z) = z^3 - 2z + 2$. On ajoutera une couleur pour les points ou l'algorithme de Newton ne converge pas.*\n",
    "\n",
    "*On peut aussi s'intéresser à $p(z) = z^5-1$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7625217",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
